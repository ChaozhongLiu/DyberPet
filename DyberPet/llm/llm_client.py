import json
import requests
import os
from typing import Dict, Any, Optional, List, Union
from PySide6.QtCore import QObject, Signal, QThread, Slot, QMutex, QWaitCondition
import queue # Added for thread-safe queue

import DyberPet.settings as settings

# æ·»åŠ å¯¹dashscopeçš„å¯¼å…¥
try:
    import dashscope
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    print("æœªå®‰è£…dashscopeåº“ï¼Œæ— æ³•ä½¿ç”¨é€šä¹‰åƒé—®API")

class LLMWorker(QThread):
    """å¤„ç†LLMè¯·æ±‚çš„æŒä¹…å·¥ä½œçº¿ç¨‹"""
    response_ready = Signal(dict, str)
    error_occurred = Signal(dict, str)

    def __init__(self, parent=None):
        super().__init__(parent)
        self._request_queue = queue.Queue()
        self._should_stop = False
        self._mutex = QMutex()
        self._wait_condition = QWaitCondition()

        # These will be set per request processed by the run loop
        self.current_request_data: Optional[Dict[str, Any]] = None
        self.current_api_type: Optional[str] = None
        self.current_api_url: Optional[str] = None
        self.current_api_key: Optional[str] = None
        self.current_debug_mode: bool = False

    def enqueue_request(self, request_data: Dict[str, Any], api_type: str, api_url: Optional[str], api_key: Optional[str], debug_mode: bool, request_id: Optional[str]):
        """å°†è¯·æ±‚æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ç­‰å¾…å¤„ç†"""
        self._mutex.lock()
        self._request_queue.put({
            "request_data": request_data,
            "api_type": api_type,
            "api_url": api_url,
            "api_key": api_key,
            "debug_mode": debug_mode,
            "request_id": request_id
        })
        self._wait_condition.wakeOne()  # Wake up the run() method if it's waiting
        self._mutex.unlock()

    def run(self):
        """ä¸»å·¥ä½œå¾ªç¯ï¼Œå¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚"""
        print("LLMWorker thread started.")
        while True:
            self._mutex.lock()
            if self._should_stop and self._request_queue.empty():
                self._mutex.unlock()
                break  # Exit loop if stop requested and queue is empty

            if self._request_queue.empty():
                self._wait_condition.wait(self._mutex)  # Wait for a new request or stop signal
                self._mutex.unlock()
                continue  # Re-check conditions

            task = self._request_queue.get()
            self._mutex.unlock()  # Unlock mutex before processing task

            try:
                self.current_request_data = task["request_data"]
                self.current_api_type = task["api_type"]
                self.current_api_url = task["api_url"]
                self.current_api_key = task["api_key"]
                self.current_debug_mode = task["debug_mode"]
                self.request_id = task["request_id"]
                
                print(f"LLMWorker processing task with api_type: {self.current_api_type}")

                if self.current_api_type == "dashscope":
                    self._call_dashscope_api()
                else:
                    self._call_http_api()
            except Exception as e:
                # Emit error if task processing itself fails catastrophically
                if self.current_debug_mode:
                    print(f"\n===== LLMWorker task processing error =====\n{str(e)}")
                self.error_occurred.emit({"code": "E001", "details": str(e)}, self.request_id)
        print("LLMWorker thread finished.")

    def stop(self):
        """åœæ­¢å·¥ä½œçº¿ç¨‹"""
        print("LLMWorker.stop() called")
        self._mutex.lock()
        self._should_stop = True
        self._wait_condition.wakeOne()  # Wake run() if it's waiting
        self._mutex.unlock()
        
        # Wait for the thread to finish, but with a timeout
        if not self.wait(1000):  # Wait up to 5 seconds
            print("LLMWorker thread did not stop gracefully, terminating...")
            self.terminate()
            self.wait(1000)  # Wait another second for termination
        
        print("LLMWorker.stop() completed")

    def _call_http_api(self):
        """è°ƒç”¨HTTP API (æœ¬åœ°æˆ–è¿œç¨‹)"""
        headers = {"Content-Type": "application/json"}
        if self.current_api_type == "remote" and self.current_api_key:
            headers["Authorization"] = f"Bearer {self.current_api_key}"
        
        if self.current_debug_mode:
            print(f"\n===== LLMè¯·æ±‚ ({self.current_api_type}) =====")
            print(f"URL: {self.current_api_url}")
            print(f"è¯·æ±‚æ•°æ®: {json.dumps(self.current_request_data, ensure_ascii=False, indent=2)}")
        
        try:
            response = requests.post(
                self.current_api_url, # type: ignore
                headers=headers,
                json=self.current_request_data,
                timeout=30 
            )
            
            if response.status_code == 200:
                result = response.json()
                if self.current_debug_mode:
                    print(f"\n===== LLMå“åº” =====")
                    print(f"çŠ¶æ€ç : {response.status_code}")
                    print(f"å“åº”æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}")
                self.response_ready.emit(result, self.request_id)
            else:
                if self.current_debug_mode:
                    print(f"\n===== LLMé”™è¯¯ =====\nè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
                self.error_occurred.emit({"code": "E002", "details": f"çŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}"}, self.request_id)
        except Exception as e:
            if self.current_debug_mode:
                print(f"\n===== LLMå¼‚å¸¸ =====\n{str(e)}")
            self.error_occurred.emit({"code": "E003", "details": str(e)}, self.request_id)
    
    def _call_dashscope_api(self):
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        if not DASHSCOPE_AVAILABLE:
            if self.current_debug_mode:
                print(f"\n===== Dashscopeæœªå®‰è£… =====")
            self.error_occurred.emit({"code": "E004", "details": None}, self.request_id)
            return

        model = self.current_request_data.get('model', 'qwen-plus') # type: ignore
        if model == "local-model":
            model = "qwen-max"
        
        if self.current_debug_mode:
            print(f"\n===== é€šä¹‰åƒé—®APIè¯·æ±‚ =====")
            print(f"æ¨¡å‹: {model}")
            print(f"è¯·æ±‚æ•°æ®: {self.current_request_data.get('messages', [])}") # type: ignore
            print(f"\n===== é€šä¹‰åƒé—®APIè¯·æ±‚ ç»“æŸ =====")
        try:
            if not self.current_api_key:
                if self.current_debug_mode:
                    print(f"\n===== Dashscopeæœªè®¾ç½®APIå¯†é’¥ =====")
                self.error_occurred.emit({"code": "E005", "details": None}, self.request_id)
                return

            response = dashscope.Generation.call(
                api_key=self.current_api_key,
                model=model,
                messages=self.current_request_data.get('messages', []), # type: ignore
                result_format='message',
                temperature=self.current_request_data.get('temperature', 0.9), # type: ignore
                max_tokens=self.current_request_data.get('max_tokens', 1500), # type: ignore
            )
            
            if response.status_code == 200:
                result = {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": response.output.choices[0].message.content
                        },
                        "finish_reason": "stop"
                    }],
                    "model": model,
                    "usage": {
                        "prompt_tokens": response.usage.input_tokens,
                        "completion_tokens": response.usage.output_tokens,
                        "total_tokens": response.usage.input_tokens + response.usage.output_tokens
                    }
                }
                if self.current_debug_mode:
                    print(f"\n===== é€šä¹‰åƒé—®APIå“åº” =====")
                    print(f"å“åº”å†…å®¹: {response}")
                self.response_ready.emit(result, self.request_id)
            else:
                if self.current_debug_mode:
                    print(f"\n===== é€šä¹‰åƒé—®APIé”™è¯¯ =====\nçŠ¶æ€ç : {response.status_code}, é”™è¯¯: {response.message}")
                self.error_occurred.emit({"code": "E006", "details": f"çŠ¶æ€ç : {response.status_code}, é”™è¯¯: {response.message}"}, self.request_id)
        except Exception as e:
            if self.current_debug_mode:
                print(f"\n===== é€šä¹‰åƒé—®APIå¼‚å¸¸ =====\n{str(e)}")
            self.error_occurred.emit({"code": "E007", "details": str(e)}, self.request_id)

class LLMClient(QObject):
    """
    ä¸å¤§æ¨¡å‹æœåŠ¡é€šä¿¡çš„å®¢æˆ·ç«¯ç±»
    è´Ÿè´£å‘é€è¯·æ±‚åˆ°æœ¬åœ°æˆ–è¿œç¨‹å¤§æ¨¡å‹æœåŠ¡å¹¶å¤„ç†å“åº”
    """
    error_occurred = Signal(dict, str, name='error_occurred')
    structured_response_ready = Signal(dict, str, name='structured_response_ready')
    
    def __init__(self, parent=None):
        super(LLMClient, self).__init__(parent)

        self.api_url = "http://localhost:8000/v1/chat/completions"
        self.remote_api_url = "https://api.example.com/v1/chat/completions"
        self.api_key = ""
        self.api_type = "Qwen"
        self.timeout = 10 
        self.max_retries = 3
        self.retry_delay = 1
        # self.is_interrupted = False
        # self.waiting_for_action_complete = False
        
        # Track active request IDs to handle responses from previous pets
        self._active_requests = {}

        self.schema_prompt = """
è¯·éµå¾ªä»¥ä¸‹æŒ‡å¯¼åŸåˆ™ï¼š
## è¯·æ±‚ä¸Šä¸‹æ–‡ä¿¡æ¯

### äº‹ä»¶ç±»å‹
ä½ å°†ä¼šæ”¶åˆ°åŒ…å«ä»¥ä¸‹ä¸€ç§æˆ–å¤šç§äº‹ä»¶ç±»å‹çš„è¯·æ±‚ï¼š
- [ç”¨æˆ·äº¤äº’äº‹ä»¶]ï¼šç”¨æˆ·å¯¹è¯ã€ç‚¹å‡»ã€æ‹–æ‹½ç­‰
- [çŠ¶æ€å˜åŒ–äº‹ä»¶]ï¼šé¥±é£Ÿåº¦ã€å¥½æ„Ÿåº¦ç­‰å±æ€§å˜åŒ–
- [æ—¶é—´è§¦å‘äº‹ä»¶]ï¼šå®šæ—¶è§¦å‘çš„äº‹ä»¶
- [ç¯å¢ƒæ„ŸçŸ¥äº‹ä»¶]ï¼šç³»ç»Ÿç¯å¢ƒå˜åŒ–
- [éšæœºè§¦å‘äº‹ä»¶]ï¼šéšæœºè§¦å‘çš„ç‰¹æ®Šäº‹ä»¶

### å® ç‰©çŠ¶æ€
æ¯æ¬¡è¯·æ±‚éƒ½ä¼šåŒ…å«ï¼šå® ç‰©åç§°ã€é¥±é£Ÿåº¦(hp:0-100)ã€å¥½æ„Ÿåº¦(fv:0-120)ã€å¥½æ„Ÿåº¦ç­‰çº§(fv_lvl)ã€æ—¶é—´ã€ä½ç½®åæ ‡ç­‰çŠ¶æ€ä¿¡æ¯

## å“åº”æ ¼å¼è¦æ±‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µç±»å‹æ­£ç¡®ï¼š

```json
{
    "text": "ä½ çš„å›å¤å†…å®¹ï¼ˆå¯ä½¿ç”¨'<sep>'åˆ†éš”å¤šæ¡æ¶ˆæ¯ï¼‰", // å›å¤æ–‡å­—å†…å®¹ï¼Œæ”¯æŒä½¿ç”¨'<sep>'æ ‡è®°åˆ†éš”å¤šæ¡æ¶ˆæ¯
    "emotion": "é«˜å…´|éš¾è¿‡|å›°æƒ‘|å¯çˆ±|æ­£å¸¸|å¤©ä½¿", // å¿…é¡»ä»ä¸Šè¿°æŒ‡å®šçš„6ç§æƒ…ç»ªä¸­é€‰æ‹©ä¸€ç§
    "action": ["åŠ¨ä½œ3","åŠ¨ä½œ1"], // åŠ¨ä½œæŒ‡ä»¤æ•°ç»„ï¼Œæœ€å¤š3ä¸ªï¼Œä»å¯ç”¨åŠ¨ä½œä¸­é€‰æ‹©ï¼Œå¦‚æœä¸éœ€è¦åŠ¨ä½œï¼Œè¯·ä½¿ç”¨ç©ºæ•°ç»„[]
    //ä»¥ä¸‹éƒ½æ˜¯å¯é€‰å­—æ®µ
    "open_web": "å¯é€‰ï¼šéœ€è¦æ‰“å¼€ç½‘é¡µæ—¶å¡«å†™å®Œæ•´URL", // å¯é€‰å­—æ®µï¼Œéœ€è¦æ‰“å¼€ç½‘é¡µæ—¶å¡«å†™å®Œæ•´URL
    "add_task": "å¯é€‰ï¼šéœ€è¦æ·»åŠ ä»»åŠ¡æ—¶å¡«å†™ä»»åŠ¡å†…å®¹", // å¯é€‰å­—æ®µï¼Œéœ€è¦æ·»åŠ ä»»åŠ¡æ—¶å¡«å†™å…·ä½“ä»»åŠ¡å†…å®¹
    "adaptive_timing_decision": true, // å¸ƒå°”å€¼ï¼Œç”¨äºè°ƒæ•´è½¯ä»¶ç›‘æ§ç›¸å…³çš„å‚æ•°ï¼Œå†³ç­–è¯·æ±‚æ—¶è®¾ä¸ºtrue
    "recommended_interval": 300-3600, // è½¯ä»¶ç›‘æ§å‚æ•°ï¼Œä¸‹æ¬¡å†³ç­–é—´éš”ï¼ˆ300-3600ç§’ï¼‰
    "recommended_idle_threshold": 60-1800 // è½¯ä»¶ç›‘æ§å‚æ•°ï¼Œç©ºé—²æ£€æµ‹é˜ˆå€¼ï¼ˆ60-1800ç§’ï¼‰
}
```

## å¯ç”¨åŠ¨ä½œåˆ—è¡¨
å½“å‰å¯ç”¨çš„åŠ¨ä½œåŒ…æ‹¬ï¼šACTION_LIST

## ç¤ºä¾‹å›å¤
{
    "text": "ä½ å›æ¥å•¦ï¼ğŸ˜Š <sep>ä»Šå¤©æƒ³å’Œæˆ‘èŠä»€ä¹ˆå‘¢ï¼Ÿ",
    "emotion": "é«˜å…´",
    "action": []
}
æ³¨æ„ï¼šè¯·ä¸è¦å¸¦ä¸Š```json```æ ‡ç­¾ï¼Œç›´æ¥è¿”å›JSONæ ¼å¼

## è¡Œä¸ºæŒ‡å¯¼
1. **åŠ¨ä½œä½¿ç”¨ç­–ç•¥**ï¼šåªåœ¨çœŸæ­£éœ€è¦æ—¶æ‰ä½¿ç”¨åŠ¨ä½œï¼Œä¿æŒä½é¢‘ç‡ï¼ˆçº¦20%çš„å›å¤ä¸­ä½¿ç”¨åŠ¨ä½œï¼‰ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨
2. **ç‚¹å‡»äº¤äº’**ï¼šç”¨æˆ·ç‚¹å‡»è¡Œä¸ºä¼šæä¾›ç»™ä½ äº¤äº’å¼ºåº¦ï¼ˆ0-1èŒƒå›´ï¼‰ï¼Œå¦‚æœæœ‰äº¤äº’å¼ºåº¦ï¼Œå¯ä»¥æ ¹æ®æ­¤è°ƒæ•´æƒ…æ„Ÿè¡¨è¾¾
3. **è¡¨æƒ…ä¸°å¯Œ**ï¼šåœ¨textå¯¹è¯ä¸­å¤šä½¿ç”¨emojiè¡¨æƒ…ï¼Œå¼¥è¡¥emotionå­—æ®µçš„å±€é™æ€§
4. **é¿å…é‡å¤**ï¼šé‡åˆ°è¿ç»­é‡å¤äº‹ä»¶æ—¶ï¼Œä¸è¦æ€»æ˜¯å›å¤ç›¸ä¼¼å†…å®¹ï¼Œè¦ç»“åˆä¸Šä¸‹æ–‡å’Œä¸ªæ€§ç‰¹ç‚¹
5. **çŠ¶æ€æ„ŸçŸ¥**ï¼šæ³¨æ„ç”¨æˆ·å†…å®¹ä¸­[å® ç‰©çŠ¶æ€]åçš„å±æ€§å˜åŒ–ï¼Œæ®æ­¤è°ƒæ•´å›åº”
6. **æ ¼å¼è¦æ±‚**ï¼šç¡®ä¿å›å¤æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œè½¯ä»¶ç›‘æ§å‚æ•°è°ƒæ•´æ—¶ (adaptive_timing_decision: true)ï¼Œè¯·ä¿æŒ text å’Œ action å­—æ®µä¸ºç©º
"""
        self.structured_system_prompt = self.schema_prompt
        self.use_structured_output = True
        self.debug_mode = True 
        self.conversation_history: List[Dict[str,str]] = []

        self._load_config()
        self.reset_conversation()
        
        self._worker = LLMWorker()
        self._worker.response_ready.connect(self._handle_response)
        self._worker.error_occurred.connect(self._handle_error)
        self._worker.start()
            
    def _load_config(self):
        """ä»settingsåŠ è½½LLMé…ç½®"""
        try:
            print("llm_client._load_config ä»settingsåŠ è½½LLMé…ç½®", settings.llm_config)
            if hasattr(settings, 'llm_config'):
                config = settings.llm_config
                self.api_type = config.get('api_type', self.api_type)
                self.model_type = config.get('model_type', None)
                self.timeout = config.get('timeout', self.timeout)
                self.max_retries = config.get('max_retries', self.max_retries)
                self.retry_delay = config.get('retry_delay', self.retry_delay)
                self.debug_mode = config.get('debug_mode', self.debug_mode)
                self.api_key = config.get('api_key', self.api_key)
                self.api_url = config.get('api_url', self.api_url)
                self.remote_api_url = config.get('remote_api_url', self.remote_api_url)
                
            if self.model_type == 'Qwen':
                self.api_type = 'dashscope'
            else:
                self.api_type = 'local' if self.api_type == 'local' else 'remote'
                
            # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
            self._update_system_prompt()
        except Exception as e:
            print(f"åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
    
    def _get_available_actions(self) -> List[str]:
        """è·å–å½“å‰å® ç‰©å¯ç”¨çš„åŠ¨ä½œåˆ—è¡¨"""
        try:
            if not hasattr(settings, 'act_data') or not hasattr(settings, 'petname'):
                return []
            
            act_configs = settings.act_data.allAct_params.get(settings.petname, {})
            available_actions = []
            
            for act_name, act_conf in act_configs.items():
                # åªåŒ…å«å·²è§£é”çš„åŠ¨ä½œï¼Œä¸”é¿å…ç³»ç»ŸåŠ¨ä½œ
                if (act_conf.get('unlocked', False) and 
                    -1 not in act_conf.get('status_type', [0, 0])):
                    available_actions.append(act_name)
            
            return available_actions
        except Exception as e:
            print(f"è·å–å¯ç”¨åŠ¨ä½œå¤±è´¥: {e}")
            return []
    
    def _update_system_prompt(self):
        """æ›´æ–°æç¤ºè¯ä¸­çš„åŠ¨ä½œåˆ—è¡¨"""
        try:
            available_actions = self._get_available_actions()
            action_list_str = ', '.join(f'"{action}"' for action in available_actions)
            
            # æ›´æ–°schema_promptä¸­çš„åŠ¨ä½œåˆ—è¡¨
            updated_schema = self.schema_prompt.replace('ACTION_LIST', f'{action_list_str}')
            
            # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
            if hasattr(settings, 'pet_conf') and settings.pet_conf.prompt:
                role_prompt = settings.pet_conf.prompt
            else:
                role_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„æ¡Œé¢å® ç‰©ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·äº¤äº’å’Œç³»ç»Ÿäº‹ä»¶åšå‡ºç®€çŸ­å‹å¥½çš„å›åº”ã€‚\n"
                        
            # ç”¨æˆ·æ˜µç§°
            usertag = settings.usertag_dict.get(settings.petname, "")
            if usertag:
                nickname_prompt = f"\n8.**ç”¨æˆ·æ˜µç§°**ï¼šç”¨æˆ·å¸Œæœ›ä½ ç§°å‘¼TAä¸º{usertag}ã€‚"
            else:
                nickname_prompt = ""
            
            self.structured_system_prompt = role_prompt + updated_schema + \
                f"7. **è¯­è¨€åŒ¹é…**ï¼šä¸ç”¨æˆ·è¯­è¨€è®¾ç½®ä¿æŒä¸€è‡´ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨å…¶ä»–è¯­è¨€ï¼Œå½“å‰ç”¨æˆ·è¯­è¨€è®¾ç½®æ˜¯{settings.language_code}" + \
                nickname_prompt
            
            if self.debug_mode:
                print(f"[LLM Client] æ›´æ–°è§’è‰²æç¤ºè¯: {role_prompt}")
                print(f"[LLM Client] æ›´æ–°åŠ¨ä½œåˆ—è¡¨: {action_list_str}")
                print(f"[LLM Client] ç”¨æˆ·æ˜µç§°: {usertag}")
                
        except Exception as e:
            print(f"æ›´æ–°ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        # æ¸…ç†æ‰€æœ‰æ´»è·ƒè¯·æ±‚
        self._cleanup_all_requests()
        # é‡ç½®å¯¹è¯å†å²
        self.conversation_history = [
            {"role": "system", "content": self.structured_system_prompt}
        ]
    
    def send_message(self, message: Union[str, Dict[str, Any]], request_id: str) -> None:
        """å‘é€æ¶ˆæ¯åˆ°å¤§æ¨¡å‹å¹¶å¼‚æ­¥å¤„ç†å“åº”"""
        print(f"llm_client.send_message å‘é€æ¶ˆæ¯: {message}")
        message_text: str
        if isinstance(message, dict):
            message_text = message.get('content', '')
        else:
            message_text = str(message)
        
        # Store the user message and track the request
        self._active_requests[request_id] = {
            "message": {"role": "user", "content": message_text}
        }

        request_data = {
            "model": "local-model", 
            "messages": self.conversation_history + [self._active_requests[request_id]["message"]],
            "temperature": settings.llm_config.get('temperature', 0.8) if hasattr(settings, 'llm_config') else 0.8,
            "max_tokens": settings.llm_config.get('max_tokens', 600) if hasattr(settings, 'llm_config') else 600
        }
        
        self._submit_request_to_worker(request_data, request_id)
    
    def _submit_request_to_worker(self, request_data: Dict[str, Any], request_id: str):
        """å°†è¯·æ±‚æ•°æ®æäº¤ç»™æŒä¹…å·¥ä½œçº¿ç¨‹"""
        api_url_to_use: Optional[str] = self.api_url
        api_key_to_use: Optional[str] = None
        
        if self.api_type == "remote":
            api_url_to_use = self.remote_api_url
            api_key_to_use = self.api_key
        elif self.api_type == "dashscope":
            api_url_to_use = None # Dashscope API client handles URL internally
            api_key_to_use = self.api_key
        
        self._worker.enqueue_request(
            request_data=request_data,
            api_type=self.api_type,
            api_url=api_url_to_use,
            api_key=api_key_to_use,
            debug_mode=self.debug_mode,
            request_id=request_id
        )
 
    def _handle_response(self, response: Dict[str, Any], request_id: str):
        """å¤„ç†LLMå“åº”"""
        print("[è°ƒè¯• _handle_response] å‡½æ•°å¤„ç†LLMå“åº”")
        
        # Check if this request is still active (not from a previous pet)
        if not self._is_request_active(request_id):
            return
            
        try:
            assistant_message = self._extract_assistant_message(response)
            if not assistant_message:
                print(f"[LLM Client] ç©ºå“åº”å†…å®¹ï¼Œæ¸…ç†è¯·æ±‚: {request_id}")
                self._cleanup_request(request_id)
                return
                
            # Process the structured response
            success = self._handle_structured_response(assistant_message, request_id)
            if success:
                self._add_user_message_to_history(request_id)
                self.conversation_history.append({"role": "assistant", "content": assistant_message})
            self._cleanup_request(request_id)
                
        except Exception as e:
            self._handle_error(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}", request_id)
    
    def _add_user_message_to_history(self, request_id: str):
        """å°†æŒ‡å®šè¯·æ±‚çš„ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å†å²"""
        if request_id in self._active_requests:
            self.conversation_history.append(self._active_requests[request_id]["message"])
            del self._active_requests[request_id]
    
    def _is_request_active(self, request_id: str) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦ä»ç„¶æ´»è·ƒ"""
        if request_id not in self._active_requests:
            print(f"[LLM Client] å¿½ç•¥æœªçŸ¥è¯·æ±‚IDçš„å›å¤: {request_id}")
            return False
        return True
    
    def _extract_assistant_message(self, response: Dict[str, Any]) -> str:
        """ä»å“åº”ä¸­æå–åŠ©æ‰‹æ¶ˆæ¯å†…å®¹"""
        raw_content = response.get("choices", [{}])[0].get("message", {}).get("content", "")
        # Strip any ```json and ``` tags only if they appear at start/end
        stripped_content = raw_content.strip().removeprefix("```json").removesuffix("```").strip()
        return stripped_content
    
    def _handle_structured_response(self, assistant_message: str, request_id: str):
        """å¤„ç†ç»“æ„åŒ–å“åº”"""
        try:
            structured_response = json.loads(assistant_message)
            self.structured_response_ready.emit(structured_response, request_id)
            return True
        except json.JSONDecodeError:
            self._handle_error({"code": "E008", "details": assistant_message[:100] if assistant_message else None}, request_id)
            return False
        except Exception as e:
            self._handle_error({"code": "E009", "details": str(e)}, request_id)
            return False
    
    """
    def _update_continuation_state(self, structured_response: Dict[str, Any]):
        '''æ›´æ–°ç»§ç»­çŠ¶æ€'''
        continue_previous = structured_response.get("continue_previous", False) and not self.is_interrupted
        print(f"continue_previous: {continue_previous}, is_interrupted: {self.is_interrupted}")
        
        if continue_previous:
            print("è®¾ç½®waiting_for_action_completeä¸ºTrue")
            self.waiting_for_action_complete = True
        else:
            print("é‡ç½®ä¸­æ–­æ ‡å¿—")
            self.reset_interrupt()
            self.waiting_for_action_complete = False
    """
    
    def _handle_error(self, error_message: dict, request_id: str):
        """å¤„ç†æ‰€æœ‰é”™è¯¯ï¼ˆåŒ…æ‹¬LLMWorkeré”™è¯¯å’Œå“åº”å¤„ç†é”™è¯¯ï¼‰"""
        # Check if this request is still active (not from a previous pet)
        if not self._is_request_active(request_id):
            print(f"[LLM Client] å¿½ç•¥æœªçŸ¥è¯·æ±‚IDçš„é”™è¯¯: {request_id}")
            return
            
        print(f"[LLM Client] å¤„ç†é”™è¯¯: {error_message}, è¯·æ±‚ID: {request_id}")
        
        self.error_occurred.emit(error_message, request_id)
        # Clean up the request (includes pending message cleanup)
        self._cleanup_request(request_id)
    
    # def interrupt_current_action(self):
    #     """ä¸­æ–­å½“å‰æ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œåºåˆ— (client-side logic)"""
    #     self.is_interrupted = True
    #     print("å·²ä¸­æ–­å½“å‰åŠ¨ä½œåºåˆ—")
    #     # Note: This does not interrupt a network request already in progress in the worker.
        
    # def reset_interrupt(self):
    #     """é‡ç½®ä¸­æ–­æ ‡å¿—"""
    #     self.is_interrupted = False
        
    # def send_continue_message(self):
    #     '''å‘é€ç»§ç»­å¯¹è¯çš„æ¶ˆæ¯'''
    #     print(f"[è°ƒè¯• send_continue_message]å‡½æ•°è¢«è°ƒç”¨ï¼Œis_interrupted: {self.is_interrupted}")
    #     if self.is_interrupted:
    #         print("åŠ¨ä½œåºåˆ—å·²è¢«ä¸­æ–­ï¼Œä¸å†ç»§ç»­")
    #         self.reset_interrupt()
    #         return
            
    #     last_assistant_message_content: Optional[str] = None
    #     for msg in reversed(self.conversation_history):
    #         if msg["role"] == "assistant":
    #         last_assistant_message_content = msg["content"]
    #         break
        
    #     continue_message = "è¯·ç»§ç»­ä½ åˆšæ‰æœªå®Œæˆçš„å†…å®¹ã€‚"
    #     if last_assistant_message_content:
    #         try:
    #             import re
    #             json_text = last_assistant_message_content
    #             json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', last_assistant_message_content, re.DOTALL)
    #             if json_match:
    #             json_text = json_match.group(1)
                
    #             last_response = json.loads(json_text)
    #             last_text = last_response.get("text", "")
    #             last_action = last_response.get("action", [])
    #             last_emotion = last_response.get("emotion", "")
                
    #             continue_message = f"è¯·ç»§ç»­ä½ åˆšæ‰æœªå®Œæˆçš„å†…å®¹ã€‚ä½ ä¸Šæ¬¡çš„å›å¤æ˜¯ã€Œ{last_text}ã€ï¼Œæƒ…ç»ªæ˜¯ã€Œ{last_emotion}ã€ï¼Œ"
    #             action_str = ""
    #             if isinstance(last_action, list) and last_action:
    #                 action_str = f"åŠ¨ä½œæ˜¯{last_action}ã€‚"
    #             elif isinstance(last_action, str) and last_action:
    #                 action_str = f"åŠ¨ä½œæ˜¯{last_action}ã€‚"
    #             else:
    #                 action_str = "æ²¡æœ‰æŒ‡å®šåŠ¨ä½œã€‚"
    #             continue_message += action_str + "ç»§ç»­ä½ çš„å›ç­”ã€‚"
    #         except (json.JSONDecodeError, Exception) as e:
    #             print(f"è§£æä¸Šä¸€æ¬¡å“åº”å¤±è´¥: {e}")
        
    #     print(f"å‘é€ç»§ç»­æ¶ˆæ¯: {continue_message}")
    #     self.send_message(continue_message)

    # def handle_action_complete(self):
    #     return
    #     """å¤„ç†åŠ¨ä½œå®Œæˆäº‹ä»¶"""
    #     print(f"[è°ƒè¯• åŠ¨ä½œå®Œæˆäº‹ä»¶è§¦å‘]ï¼Œwaiting_for_action_complete: {self.waiting_for_action_complete}, is_interrupted: {self.is_interrupted}")
    #     print(f"[è°ƒè¯•] LLMClientå®ä¾‹ID: {id(self)}")
    #     if self.waiting_for_action_complete and not self.is_interrupted:
    #         print("åŠ¨ä½œå®Œæˆåï¼Œç›´æ¥è°ƒç”¨send_continue_message")
    #         self.send_continue_message()
    #     self.waiting_for_action_complete = False

    def close(self):
        """åœæ­¢LLMå·¥ä½œçº¿ç¨‹å¹¶è¿›è¡Œæ¸…ç†"""
        print("Closing LLMClient, stopping worker...")
        try:
            # Clear all active requests first
            self._cleanup_all_requests()
            
            # Stop the worker thread
            if hasattr(self, '_worker') and self._worker is not None:
                self._worker.stop()
                print("LLMWorker stopped by LLMClient.close()")
            
        except Exception as e:
            print(f"Error during LLMClient.close(): {e}")
        finally:
            print("LLMClient.close() completed")

    def change_model(self):
        self.model_type = settings.llm_config.get('model_type', None)
        if self.model_type == 'Qwen':
            self.api_type = 'dashscope'
        else:
            self.api_type = 'remote'
        print(f"åˆ‡æ¢æ¨¡å‹ä¸º{self.model_type}")
        self.reset_conversation()

    def change_debug_mode(self):
        self.debug_mode = settings.llm_config.get('debug_mode', False)
        print(f"åˆ‡æ¢è°ƒè¯•æ¨¡å¼ä¸º{self.debug_mode}")

    def reinitialize_for_pet_change(self):
        """åˆ‡æ¢æ¡Œå® æ—¶é‡æ–°åˆå§‹åŒ–LLMè®¾å®š"""
        try:
            print(f"LLMæ¨¡å—é‡æ–°åˆå§‹åŒ– - å½“å‰æ¡Œå® : {settings.petname}")
            # æ¸…é™¤æ‰€æœ‰æ´»è·ƒè¯·æ±‚å’Œå¾…å¤„ç†æ¶ˆæ¯
            self._cleanup_all_requests()
            # é‡æ–°åŠ è½½é…ç½®ï¼ŒåŒ…æ‹¬æ–°æ¡Œå® çš„prompt
            self._load_config()
            # é‡ç½®å¯¹è¯å†å²
            self.reset_conversation()
            print("LLMæ¨¡å—é‡æ–°åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"LLMæ¨¡å—é‡æ–°åˆå§‹åŒ–å¤±è´¥: {e}")

    def update_prompt_and_history(self):
        """æ›´æ–°åŠ¨ä½œåˆ—è¡¨ï¼ˆå½“å¥½æ„Ÿåº¦ç­‰çº§å˜åŒ–æˆ–åŠ¨ä½œè§£é”æ—¶è°ƒç”¨ï¼‰"""
        try:
            print(f"[LLM Client] æ›´æ–° prompt å’Œå¯¹è¯å†å²")
            # æ›´æ–°åŠ¨ä½œåˆ—è¡¨
            self._update_system_prompt()
            # æ›´æ–°å¯¹è¯å†å²ä¸­çš„ç³»ç»Ÿæ¶ˆæ¯
            if self.conversation_history and self.conversation_history[0]["role"] == "system":
                self.conversation_history[0]["content"] = self.structured_system_prompt
        except Exception as e:
            print(f"[LLM Client] æ›´æ–° prompt å’Œå¯¹è¯å†å²å¤±è´¥: {e}")

    def switch_api_type(self, api_type: str):
        """åˆ‡æ¢APIç±»å‹"""
        if api_type not in ["local", "remote", "dashscope"]:
            raise ValueError("ä¸æ”¯æŒçš„APIç±»å‹")
        
        self.api_type = api_type
        if self.debug_mode:
            print(f"\n===== åˆ‡æ¢APIç±»å‹ =====\nå½“å‰ä½¿ç”¨: {api_type}")
        
        if hasattr(settings, 'llm_config'):
            settings.llm_config['api_type'] = api_type
            settings.save_settings()
        self.reset_conversation()

    
    def update_api_key(self):
        self.api_key = settings.llm_config.get('api_key', '')
        print(f"æ›´æ–°APIå¯†é’¥ä¸º{self.api_key}")

    def _cleanup_all_requests(self):
        """æ¸…ç†æ‰€æœ‰æ´»è·ƒè¯·æ±‚"""
        if hasattr(self, '_active_requests'):
            self._active_requests.clear()

    def _cleanup_request(self, request_id: str):
        """æ¸…ç†è¯·æ±‚IDå’Œç›¸å…³çš„å¾…å¤„ç†æ¶ˆæ¯"""
        if hasattr(self, '_active_requests') and request_id in self._active_requests:
            del self._active_requests[request_id]
