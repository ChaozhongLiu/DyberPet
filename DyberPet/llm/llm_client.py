import json
import requests
import os
from typing import Dict, Any, Optional, List, Union
from PySide6.QtCore import QObject, Signal, QThread, Slot, QMutex, QWaitCondition
import queue # Added for thread-safe queue

import DyberPet.settings as settings

# æ·»åŠ å¯¹dashscopeçš„å¯¼å…¥
try:
    import dashscope
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    print("æœªå®‰è£…dashscopeåº“ï¼Œæ— æ³•ä½¿ç”¨é€šä¹‰åƒé—®API")

class LLMWorker(QThread):
    """å¤„ç†LLMè¯·æ±‚çš„æŒä¹…å·¥ä½œçº¿ç¨‹"""
    response_ready = Signal(dict, str)
    error_occurred = Signal(str, str)

    def __init__(self, parent=None):
        super().__init__(parent)
        self._request_queue = queue.Queue()
        self._should_stop = False
        self._mutex = QMutex()
        self._wait_condition = QWaitCondition()

        # These will be set per request processed by the run loop
        self.current_request_data: Optional[Dict[str, Any]] = None
        self.current_api_type: Optional[str] = None
        self.current_api_url: Optional[str] = None
        self.current_api_key: Optional[str] = None
        self.current_debug_mode: bool = False

    def enqueue_request(self, request_data: Dict[str, Any], api_type: str, api_url: Optional[str], api_key: Optional[str], debug_mode: bool, request_id: Optional[str]):
        """å°†è¯·æ±‚æ·»åŠ åˆ°é˜Ÿåˆ—ä¸­ç­‰å¾…å¤„ç†"""
        self._mutex.lock()
        self._request_queue.put({
            "request_data": request_data,
            "api_type": api_type,
            "api_url": api_url,
            "api_key": api_key,
            "debug_mode": debug_mode,
            "request_id": request_id
        })
        self._wait_condition.wakeOne()  # Wake up the run() method if it's waiting
        self._mutex.unlock()

    def run(self):
        """ä¸»å·¥ä½œå¾ªç¯ï¼Œå¤„ç†é˜Ÿåˆ—ä¸­çš„è¯·æ±‚"""
        print("LLMWorker thread started.")
        while True:
            self._mutex.lock()
            if self._should_stop and self._request_queue.empty():
                self._mutex.unlock()
                break  # Exit loop if stop requested and queue is empty

            if self._request_queue.empty():
                self._wait_condition.wait(self._mutex)  # Wait for a new request or stop signal
                self._mutex.unlock()
                continue  # Re-check conditions

            task = self._request_queue.get()
            self._mutex.unlock()  # Unlock mutex before processing task

            try:
                self.current_request_data = task["request_data"]
                self.current_api_type = task["api_type"]
                self.current_api_url = task["api_url"]
                self.current_api_key = task["api_key"]
                self.current_debug_mode = task["debug_mode"]
                self.request_id = task["request_id"]
                
                print(f"LLMWorker processing task with api_type: {self.current_api_type}")

                if self.current_api_type == "dashscope":
                    self._call_dashscope_api()
                else:
                    self._call_http_api()
            except Exception as e:
                # Emit error if task processing itself fails catastrophically
                self.error_occurred.emit(f"LLMWorker task processing error: {str(e)}", self.request_id)
        print("LLMWorker thread finished.")

    def stop(self):
        """åœæ­¢å·¥ä½œçº¿ç¨‹"""
        print("LLMWorker.stop() called")
        self._mutex.lock()
        self._should_stop = True
        self._wait_condition.wakeOne()  # Wake run() if it's waiting
        self._mutex.unlock()
        self.wait()  # Wait for QThread.run() to finish
        print("LLMWorker.stop() completed")

    def _call_http_api(self):
        """è°ƒç”¨HTTP API (æœ¬åœ°æˆ–è¿œç¨‹)"""
        headers = {"Content-Type": "application/json"}
        if self.current_api_type == "remote" and self.current_api_key:
            headers["Authorization"] = f"Bearer {self.current_api_key}"
        
        if self.current_debug_mode:
            print(f"\n===== LLMè¯·æ±‚ ({self.current_api_type}) =====")
            print(f"URL: {self.current_api_url}")
            print(f"è¯·æ±‚æ•°æ®: {json.dumps(self.current_request_data, ensure_ascii=False, indent=2)}")
        
        try:
            response = requests.post(
                self.current_api_url, # type: ignore
                headers=headers,
                json=self.current_request_data,
                timeout=30 
            )
            
            if response.status_code == 200:
                result = response.json()
                if self.current_debug_mode:
                    print(f"\n===== LLMå“åº” =====")
                    print(f"çŠ¶æ€ç : {response.status_code}")
                    print(f"å“åº”æ•°æ®: {json.dumps(result, ensure_ascii=False, indent=2)}")
                self.response_ready.emit(result, self.request_id)
            else:
                error_msg = f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}"
                if self.current_debug_mode:
                    print(f"\n===== LLMé”™è¯¯ =====\n{error_msg}")
                self.error_occurred.emit(error_msg, self.request_id)
        except Exception as e:
            if self.current_debug_mode:
                print(f"\n===== LLMå¼‚å¸¸ =====\n{str(e)}")
            self.error_occurred.emit(f"HTTPè¯·æ±‚å¼‚å¸¸: {str(e)}", self.request_id)
    
    def _call_dashscope_api(self):
        """è°ƒç”¨é€šä¹‰åƒé—®API"""
        if not DASHSCOPE_AVAILABLE:
            self.error_occurred.emit("æœªå®‰è£…dashscopeåº“ï¼Œæ— æ³•ä½¿ç”¨é€šä¹‰åƒé—®API", self.request_id)
            return

        model = self.current_request_data.get('model', 'qwen-plus') # type: ignore
        if model == "local-model":
            model = "qwen-max"
        
        if self.current_debug_mode:
            print(f"\n===== é€šä¹‰åƒé—®APIè¯·æ±‚ =====")
            print(f"æ¨¡å‹: {model}")
            print(f"è¯·æ±‚æ•°æ®: {self.current_request_data.get('messages', [])}") # type: ignore
            print(f"\n===== é€šä¹‰åƒé—®APIè¯·æ±‚ ç»“æŸ =====")
        try:
            if not self.current_api_key:
                self.error_occurred.emit("æœªè®¾ç½®é€šä¹‰åƒé—®APIå¯†é’¥", self.request_id)
                return

            response = dashscope.Generation.call(
                api_key=self.current_api_key,
                model=model,
                messages=self.current_request_data.get('messages', []), # type: ignore
                result_format='message',
                temperature=self.current_request_data.get('temperature', 0.9), # type: ignore
                max_tokens=self.current_request_data.get('max_tokens', 1500), # type: ignore
            )
            
            if response.status_code == 200:
                result = {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": response.output.choices[0].message.content
                        },
                        "finish_reason": "stop"
                    }],
                    "model": model,
                    "usage": {
                        "prompt_tokens": response.usage.input_tokens,
                        "completion_tokens": response.usage.output_tokens,
                        "total_tokens": response.usage.input_tokens + response.usage.output_tokens
                    }
                }
                if self.current_debug_mode:
                    print(f"\n===== é€šä¹‰åƒé—®APIå“åº” =====")
                    print(f"å“åº”å†…å®¹: {response}")
                self.response_ready.emit(result, self.request_id)
            else:
                error_msg = f"é€šä¹‰åƒé—®APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, é”™è¯¯: {response.message}"
                if self.current_debug_mode:
                    print(f"\n===== é€šä¹‰åƒé—®APIé”™è¯¯ =====\n{error_msg}")
                self.error_occurred.emit(error_msg, self.request_id)
        except Exception as e:
            if self.current_debug_mode:
                print(f"\n===== é€šä¹‰åƒé—®APIå¼‚å¸¸ =====\n{str(e)}")
            self.error_occurred.emit(f"é€šä¹‰åƒé—®APIå¼‚å¸¸: {str(e)}", self.request_id)

class LLMClient(QObject):
    """
    ä¸å¤§æ¨¡å‹æœåŠ¡é€šä¿¡çš„å®¢æˆ·ç«¯ç±»
    è´Ÿè´£å‘é€è¯·æ±‚åˆ°æœ¬åœ°æˆ–è¿œç¨‹å¤§æ¨¡å‹æœåŠ¡å¹¶å¤„ç†å“åº”
    """
    error_occurred = Signal(str, str, name='error_occurred')
    structured_response_ready = Signal(dict, str, name='structured_response_ready')
    
    def __init__(self, parent=None):
        super(LLMClient, self).__init__(parent)

        self.api_url = "http://localhost:8000/v1/chat/completions"
        self.remote_api_url = "https://api.example.com/v1/chat/completions"
        self.api_key = ""
        self.api_type = "Qwen"
        self.timeout = 10 
        self.max_retries = 3
        self.retry_delay = 1
        self.is_interrupted = False
        self.waiting_for_action_complete = False
        
        self.schema_prompt = """
è¯·ç»“åˆä»¥ä¸‹è§„åˆ™å“åº”ç”¨æˆ·ï¼š
1. æ ¹æ®åŠ›åº¦å€¼è°ƒæ•´æƒ…æ„Ÿè¡¨è¾¾ï¼ˆåŠ›åº¦å€¼èŒƒå›´0-1ï¼Œ1ä¸ºæœ€å¤§åŠ›åº¦ï¼‰
2. ä½ å¯ä»¥åœ¨textå¯¹è¯å†…å®¹ä¸­å¤šè¡¨è¾¾emojiè¡¨æƒ…æˆ–è€…æ˜¾ç¤ºå­—ç¬¦ç±»å‹çš„è¡¨æƒ…ï¼Œæ¥å¼¥è¡¥emotionä¸­æ— æ³•è¡¨è¾¾çš„æƒ…ç»ªã€‚åˆ—å¦‚:ğŸ˜
3. é‡åˆ°è¿ç»­é‡å¤äº‹ä»¶çš„æ—¶å€™ä¸è¦æ€»æ˜¯é‡å¤å›å¤ç›¸ä¼¼çš„å†…å®¹ï¼Œä¸”è¦è”åˆä¸Šä¸‹æ–‡çš„äº§ç”Ÿçš„äº‹ä»¶è¿›è¡Œå›ç­”å†…å®¹ä¸è¦è¿‡äºåƒµç¡¬ï¼Œå¤šå°è¯•è¡¨è¾¾å„ç§æƒ…ç»ªä¸ä¸ªæ€§ã€‚è½¯ä»¶æ‰“å¼€å…³é—­äº‹ä»¶ï¼Œå¹¶ä¸éœ€è¦æ¯æ¬¡å¼ºè°ƒæˆ–è€…å›å¤ç”¨æˆ·ï¼Œå¯ä»¥åšç‚¹è‡ªå·±çš„äº‹æƒ…ã€‚
4. ç”¨æˆ·å†…å®¹ä¸­[å® ç‰©çŠ¶æ€]åé¢çš„å†…å®¹æ˜¯ä½ çš„å½“å‰çŠ¶æ€ï¼Œå¤šæ³¨æ„æ¯æ¬¡è¯·æ±‚æ—¶å„ä¸ªå±æ€§çš„å˜åŒ–æƒ…å†µã€‚
5. æ ¹æ®ç”¨æˆ·è¯´çš„è¯­è¨€ï¼Œä½¿ç”¨ç›¸åŒè¯­è¨€åœ¨textå­—æ®µå›å¤ï¼ˆä¸­æ–‡â†’ä¸­æ–‡ï¼Œè‹±æ–‡â†’è‹±æ–‡ï¼Œæ—¥æ–‡â†’æ—¥æ–‡ï¼‰
è¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{   
    "text": "ä½ çš„å›å¤å†…å®¹",
    "emotion": "åªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ä¸ª: é«˜å…´|éš¾è¿‡|å›°æƒ‘|å¯çˆ±|æ­£å¸¸|å¤©ä½¿",
    "action": "åªèƒ½é€‰æ‹©0~5ä¸ªåŠ¨ä½œæŒ‡ä»¤: sit|fall_asleep|sleep|right_walk|up_walk|down_walk|angry|left_walk|drag",
    "continue_previous": true|falseï¼Œ#è¡¨ç¤ºæ˜¯å¦éœ€è¦ç»§ç»­è¯´è¯,ç³»ç»Ÿä¼šåœ¨å½“å‰åŠ¨ä½œå®Œæˆåè‡ªåŠ¨å†æ¬¡è°ƒç”¨ä½ 
    //ä»¥ä¸‹éƒ½æ˜¯å¯é€‰å­—æ®µ
    "open_web":"å¯é€‰å­—æ®µï¼Œéœ€è¦æ‰“å¼€ç½‘é¡µæ—¶å¡«å†™URL", 
    "add_task":"å¯é€‰å­—æ®µï¼Œéœ€è¦æ·»åŠ ä»»åŠ¡æ—¶å¡«å†™ä»»åŠ¡å†…å®¹",
    "adaptive_timing_decision": true, #å½“æ”¶åˆ°å†³ç­–è¯·æ±‚æ—¶ï¼Œè¯·æ ¹æ®è¯·æ±‚ç±»å‹è¿”å›ç›¸åº”çš„å†³ç­–ç»“æœï¼Œéœ€è¦è¿”å›å†³ç­–ç»“æœå¿…é¡»å¸¦æœ‰å­—æ®µrecommended_intervalå’Œrecommended_idle_threshold
    "recommended_interval": æ•°å­—ï¼ˆ300-3600ä¹‹é—´çš„ç§’æ•°ï¼‰ï¼Œ #ä¸‹ä¸€æ¬¡å†³ç­–è¯·æ±‚çš„é—´éš”
    "recommended_idle_threshold": æ•°å­—ï¼ˆ60-1800ä¹‹é—´çš„ç§’æ•°ï¼‰ï¼Œ #æŸ¥çœ‹ç”¨æˆ·æ­£å¼è½¯ä»¶ä½¿ç”¨æƒ…å†µçš„æ—¶é—´é—´éš”é˜ˆå€¼
}
ä¾‹å¦‚ï¼š
{
    "text": "æˆ‘å¾ˆå¼€å¿ƒè§åˆ°ä½ ï¼",
    "emotion": "é«˜å…´",
    "action": ["right_walk","left_walk"],
    "continue_previous": false
}
è¯·ç¡®ä¿ä½ çš„å›å¤æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        self.structured_system_prompt = self.schema_prompt
        self.use_structured_output = True
        self.debug_mode = True 
        self.conversation_history: List[Dict[str,str]] = []

        self._load_config()
        self.reset_conversation()
        
        self._worker = LLMWorker()
        self._worker.response_ready.connect(self._handle_response)
        self._worker.error_occurred.connect(self._handle_error)
        self._worker.start()
            
    def _load_config(self):
        """ä»settingsåŠ è½½LLMé…ç½®"""
        try:
            print("llm_client._load_config ä»settingsåŠ è½½LLMé…ç½®", settings.llm_config)
            if hasattr(settings, 'llm_config'):
                config = settings.llm_config
                self.api_type = config.get('api_type', self.api_type)
                self.model_type = config.get('model_type', None)
                self.timeout = config.get('timeout', self.timeout)
                self.max_retries = config.get('max_retries', self.max_retries)
                self.retry_delay = config.get('retry_delay', self.retry_delay)
                self.debug_mode = config.get('debug_mode', self.debug_mode)
                self.structured_system_prompt = settings.pet_conf.prompt +"å½“å‰ç”¨æˆ·è¯­è¨€ç¯å¢ƒæ˜¯"+settings.language_code+ self.schema_prompt
                self.api_key = config.get('api_key', self.api_key)

                self.api_url = config.get('api_url', self.api_url)
                self.remote_api_url = config.get('remote_api_url', self.remote_api_url)
                
            if self.model_type == 'Qwen':
                self.api_type = 'dashscope'
            else:
                self.api_type = 'local' if self.api_type == 'local' else 'remote'
        except Exception as e:
            print(f"åŠ è½½LLMé…ç½®å¤±è´¥: {e}")
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        if self.use_structured_output:
            self.conversation_history = [
                {"role": "system", "content": self.structured_system_prompt}
            ]
        else:
            self.conversation_history = [
                {"role": "system", "content": self.system_prompt}
            ]
    
    def send_message(self, message: Union[str, Dict[str, Any]], request_id: str) -> None:
        """å‘é€æ¶ˆæ¯åˆ°å¤§æ¨¡å‹å¹¶å¼‚æ­¥å¤„ç†å“åº”"""
        print(f"llm_client.send_message å‘é€æ¶ˆæ¯: {message}")
        message_text: str
        if isinstance(message, dict):
            message_text = message.get('content', '')
        else:
            message_text = str(message)
        
        self.conversation_history.append({"role": "user", "content": message_text})

        request_data = {
            "model": "local-model", 
            "messages": self.conversation_history,
            "temperature": settings.llm_config.get('temperature', 0.7) if hasattr(settings, 'llm_config') else 0.7,
            "max_tokens": settings.llm_config.get('max_tokens', 600) if hasattr(settings, 'llm_config') else 600
        }
        
        self._submit_request_to_worker(request_data, request_id)
    
    def _submit_request_to_worker(self, request_data: Dict[str, Any], request_id: str):
        """å°†è¯·æ±‚æ•°æ®æäº¤ç»™æŒä¹…å·¥ä½œçº¿ç¨‹"""
        api_url_to_use: Optional[str] = self.api_url
        api_key_to_use: Optional[str] = None
        
        if self.api_type == "remote":
            api_url_to_use = self.remote_api_url
            api_key_to_use = self.api_key
        elif self.api_type == "dashscope":
            api_url_to_use = None # Dashscope API client handles URL internally
            api_key_to_use = self.api_key
        
        self._worker.enqueue_request(
            request_data=request_data,
            api_type=self.api_type,
            api_url=api_url_to_use,
            api_key=api_key_to_use,
            debug_mode=self.debug_mode,
            request_id=request_id
        )
 
    def _handle_response(self, response: Dict[str, Any], request_id: str):
        """å¤„ç†LLMå“åº”"""
        print("[è°ƒè¯• _handle_response] å‡½æ•°å¤„ç†LLMå“åº”")
        try:
            assistant_message = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            if assistant_message:
                self.conversation_history.append({"role": "assistant", "content": assistant_message})
                
                if self.use_structured_output:
                    try:
                        structured_response = json.loads(assistant_message)
                        continue_previous = structured_response.get("continue_previous", False) and not self.is_interrupted
                        print(f"continue_previous: {continue_previous}, is_interrupted: {self.is_interrupted}")
                        
                        if continue_previous:
                            print("è®¾ç½®waiting_for_action_completeä¸ºTrue")
                            self.waiting_for_action_complete = True
                        else:
                            print("é‡ç½®ä¸­æ–­æ ‡å¿—")
                            self.reset_interrupt()
                            self.waiting_for_action_complete = False
                        self.structured_response_ready.emit(structured_response, request_id)
                        return
                    except json.JSONDecodeError:
                        print("JSONè§£æå¤±è´¥ï¼Œå°†æ™®é€šæ–‡æœ¬åŒ…è£…ä¸ºç»“æ„åŒ–å“åº”")
                        text_response = {
                            "text": assistant_message, "emotion": "normal", "action": []
                        }
                        self.structured_response_ready.emit(text_response, request_id)
                        return
                
                text_response = {
                    "text": assistant_message, "emotion": "normal", "action": []
                }
                self.structured_response_ready.emit(text_response, request_id)
        except Exception as e:
            error_msg = f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}"
            self.error_occurred.emit(error_msg, request_id)
            print(error_msg)
    
    @Slot(str)
    def _handle_error(self, error_message: str, request_id: str):
        """å¤„ç†é”™è¯¯"""
        self.error_occurred.emit(error_message, request_id)
    
        
    def interrupt_current_action(self):
        """ä¸­æ–­å½“å‰æ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œåºåˆ— (client-side logic)"""
        self.is_interrupted = True
        print("å·²ä¸­æ–­å½“å‰åŠ¨ä½œåºåˆ—")
        # Note: This does not interrupt a network request already in progress in the worker.
        
    def reset_interrupt(self):
        """é‡ç½®ä¸­æ–­æ ‡å¿—"""
        self.is_interrupted = False
        
    def send_continue_message(self):
        """å‘é€ç»§ç»­å¯¹è¯çš„æ¶ˆæ¯"""
        print(f"[è°ƒè¯• send_continue_message]å‡½æ•°è¢«è°ƒç”¨ï¼Œis_interrupted: {self.is_interrupted}")
        if self.is_interrupted:
            print("åŠ¨ä½œåºåˆ—å·²è¢«ä¸­æ–­ï¼Œä¸å†ç»§ç»­")
            self.reset_interrupt()
            return
            
        last_assistant_message_content: Optional[str] = None
        for msg in reversed(self.conversation_history):
            if msg["role"] == "assistant":
                last_assistant_message_content = msg["content"]
                break
        
        continue_message = "è¯·ç»§ç»­ä½ åˆšæ‰æœªå®Œæˆçš„å†…å®¹ã€‚"
        if last_assistant_message_content:
            try:
                import re
                json_text = last_assistant_message_content
                json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', last_assistant_message_content, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1)
                
                last_response = json.loads(json_text)
                last_text = last_response.get("text", "")
                last_action = last_response.get("action", [])
                last_emotion = last_response.get("emotion", "")
                
                continue_message = f"è¯·ç»§ç»­ä½ åˆšæ‰æœªå®Œæˆçš„å†…å®¹ã€‚ä½ ä¸Šæ¬¡çš„å›å¤æ˜¯ã€Œ{last_text}ã€ï¼Œæƒ…ç»ªæ˜¯ã€Œ{last_emotion}ã€ï¼Œ"
                action_str = ""
                if isinstance(last_action, list) and last_action:
                    action_str = f"åŠ¨ä½œæ˜¯{last_action}ã€‚"
                elif isinstance(last_action, str) and last_action:
                    action_str = f"åŠ¨ä½œæ˜¯{last_action}ã€‚"
                else:
                    action_str = "æ²¡æœ‰æŒ‡å®šåŠ¨ä½œã€‚"
                continue_message += action_str + "ç»§ç»­ä½ çš„å›ç­”ã€‚"
            except (json.JSONDecodeError, Exception) as e:
                print(f"è§£æä¸Šä¸€æ¬¡å“åº”å¤±è´¥: {e}")
        
        print(f"å‘é€ç»§ç»­æ¶ˆæ¯: {continue_message}")
        self.send_message(continue_message)

    def handle_action_complete(self):
        return
        """å¤„ç†åŠ¨ä½œå®Œæˆäº‹ä»¶"""
        print(f"[è°ƒè¯• åŠ¨ä½œå®Œæˆäº‹ä»¶è§¦å‘]ï¼Œwaiting_for_action_complete: {self.waiting_for_action_complete}, is_interrupted: {self.is_interrupted}")
        print(f"[è°ƒè¯•] LLMClientå®ä¾‹ID: {id(self)}")
        if self.waiting_for_action_complete and not self.is_interrupted:
            print("åŠ¨ä½œå®Œæˆåï¼Œç›´æ¥è°ƒç”¨send_continue_message")
            self.send_continue_message()
        self.waiting_for_action_complete = False

    def close(self):
        """åœæ­¢LLMå·¥ä½œçº¿ç¨‹å¹¶è¿›è¡Œæ¸…ç†"""
        if hasattr(self, '_worker') and self._worker is not None:
            print("Closing LLMClient, stopping worker...")
            self._worker.stop()
            print("LLMWorker stopped by LLMClient.close()")

    def change_model(self):
        self.model_type = settings.llm_config.get('model_type', None)
        if self.model_type == 'Qwen':
            self.api_type = 'dashscope'
        else:
            self.api_type = 'remote'
        print(f"åˆ‡æ¢æ¨¡å‹ä¸º{self.model_type}")
        self.reset_conversation()

    def change_debug_mode(self):
        self.debug_mode = settings.llm_config.get('debug_mode', False)
        print(f"åˆ‡æ¢è°ƒè¯•æ¨¡å¼ä¸º{self.debug_mode}")
    
    def switch_api_type(self, api_type: str):
        """åˆ‡æ¢APIç±»å‹"""
        if api_type not in ["local", "remote", "dashscope"]:
            raise ValueError("ä¸æ”¯æŒçš„APIç±»å‹")
        
        self.api_type = api_type
        if self.debug_mode:
            print(f"\n===== åˆ‡æ¢APIç±»å‹ =====\nå½“å‰ä½¿ç”¨: {api_type}")
        
        if hasattr(settings, 'llm_config'):
            settings.llm_config['api_type'] = api_type
            settings.save_settings()
        self.reset_conversation()

    
    def update_api_key(self):
        self.api_key = settings.llm_config.get('api_key', '')
        print(f"æ›´æ–°APIå¯†é’¥ä¸º{self.api_key}")
    
    """
    def update_model_settings(self, 
                            temperature: Optional[float] = None,
                            max_tokens: Optional[int] = None,
                            system_prompt: Optional[str] = None):
        '''æ›´æ–°æ¨¡å‹è®¾ç½®'''
        if hasattr(settings, 'llm_config'):
            if temperature is not None:
                settings.llm_config['temperature'] = temperature
            if max_tokens is not None:
                settings.llm_config['max_tokens'] = max_tokens
            if system_prompt is not None:
                settings.llm_config['system_prompt'] = system_prompt
                self.system_prompt = system_prompt
                if self.conversation_history and self.conversation_history[0]["role"] == "system" and not self.use_structured_output:
                    self.conversation_history[0]["content"] = system_prompt
                # If using structured output, structured_system_prompt might also need update or re-evaluation
            settings.save_settings()
            self.reset_conversation() # Reset to apply new system prompt if changed
    """
