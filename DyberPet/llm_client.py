import json
import requests
import os
from typing import Dict, Any, Optional, List, Union
from PySide6.QtCore import QObject, Signal, QThread, Slot


from . import settings

# æ·»åŠ å¯¹dashscopeçš„å¯¼å…¥
try:
    import dashscope
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    print("DashScope library not installed, cannot use Qwen API")

class LLMWorker(QThread):
    """å¤„ç†LLMè¯·æ±‚çš„å·¥ä½œçº¿ç¨‹"""
    response_ready = Signal(dict)
    error_occurred = Signal(str)
    
    def __init__(self, request_data, api_type, api_url=None, api_key=None, debug_mode=False):
        super().__init__()
        self.request_data = request_data
        self.api_type = api_type  # "local", "remote", "dashscope"
        self.api_url = api_url
        self.api_key = api_key
        self.timeout = 10  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.retry_delay = 1  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.debug_mode = debug_mode
        print(f"LLMWorker initialized with api_type: {self.api_type}")
    
    def run(self):
        try:
            if self.api_type == "dashscope":
                self._call_dashscope_api()
            else:
                self._call_http_api()
        except Exception as e:
            self.error_occurred.emit(f"LLMè¯·æ±‚å‡ºé”™: {str(e)}")
    
    def _call_http_api(self):
        """è°ƒç”¨HTTP API (æœ¬åœ°æˆ–è¿œç¨‹)"""
        headers = {"Content-Type": "application/json"}
        if self.api_type == "remote" and self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        if self.debug_mode:
            print(f"\n===== LLM Request ({self.api_type}) =====")
            print(f"URL: {self.api_url}")
            print(f"Request Data: {json.dumps(self.request_data, ensure_ascii=False, indent=2)}")
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=self.request_data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if self.debug_mode:
                    print(f"\n===== LLM Response =====")
                    print(f"Status Code: {response.status_code}")
                    print(f"Response Data: {json.dumps(result, ensure_ascii=False, indent=2)}")
                self.response_ready.emit(result)
            else:
                error_msg = f"Request failed, status code: {response.status_code}, response: {response.text}"
                if self.debug_mode:
                    print(f"\n===== LLM Error =====\n{error_msg}")
                self.error_occurred.emit(error_msg)
        except Exception as e:
            if self.debug_mode:
                print(f"\n===== LLM Exception =====\n{str(e)}")
            self.error_occurred.emit(f"HTTP request exception: {str(e)}")
    
    def _call_dashscope_api(self):
        """Call DashScope API"""
        if not DASHSCOPE_AVAILABLE:
            self.error_occurred.emit("DashScope library not installed, cannot use Qwen API")
            return

        # Prepare request parameters
        model = self.request_data.get('model', 'qwen-plus')
        if model == "local-model":
            model = "qwen-max"  # Default to qwen-max


        if self.debug_mode:
            print(f"\n===== DashScope API Request =====")
            print(f"Model: {model}")
            # print(f"Messages: {json.dumps(self.request_data.get('messages', []), ensure_ascii=False, indent=2)}")
            print(f"Request Data: {self.request_data.get('messages', [])}")
            print(f"\n===== DashScope API Request End =====")
        try:
            # Set API key
            if not self.api_key:
                self.error_occurred.emit("DashScope API key not set")
                return

            # è°ƒç”¨API
            response = dashscope.Generation.call(
                api_key=self.api_key,
                model=model,
                messages=self.request_data.get('messages', []),
                result_format='message',  # ä½¿ç”¨messageæ ¼å¼
                temperature=self.request_data.get('temperature', 0.9),
                max_tokens=self.request_data.get('max_tokens', 1500),
            )
            
            # å¤„ç†å“åº”
            if response.status_code == 200:
                # è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„å“åº”
                result = {
                    "choices": [{
                        "message": {
                            "role": "assistant",
                            "content": response.output.choices[0].message.content
                        },
                        "finish_reason": "stop"
                    }],
                    "model": model,
                    "usage": {
                        "prompt_tokens": response.usage.input_tokens,
                        "completion_tokens": response.usage.output_tokens,
                        "total_tokens": response.usage.input_tokens + response.usage.output_tokens
                    }
                }
                
                if self.debug_mode:
                    print(f"\n===== DashScope API Response =====")
                    print(f"Response Content: {response}") #response.output.choices[0].message.content

                self.response_ready.emit(result)
            else:
                error_msg = f"DashScope API request failed, status code: {response.status_code}, error: {response.message}"
                if self.debug_mode:
                    print(f"\n===== DashScope API Error =====\n{error_msg}")
                self.error_occurred.emit(error_msg)
        except Exception as e:
            if self.debug_mode:
                print(f"\n===== DashScope API Exception =====\n{str(e)}")
            self.error_occurred.emit(f"DashScope API exception: {str(e)}")

class LLMClient(QObject):
    """
    Client class for communicating with large language model services
    Responsible for sending requests to local or remote LLM services and handling responses
    """
    
    # å®šä¹‰ä¿¡å·ï¼Œç”¨äºé€šçŸ¥UIå±‚å¤§æ¨¡å‹å“åº”å·²ç»å‡†å¤‡å¥½
    response_ready = Signal(str, name='response_ready')
    error_occurred = Signal(str, name='error_occurred')
    structured_response_ready = Signal(dict, name='structured_response_ready')
    
    def __init__(self, parent=None):
        super(LLMClient, self).__init__(parent)
        # Default configuration, can be overridden by settings.json
        self.api_url = "http://localhost:8000/v1/chat/completions"  # Default local service address
        self.remote_api_url = "https://api.example.com/v1/chat/completions"  # Remote API address
        self.api_key = ""  # API key
        self.api_type = "local"  # Default to local model, options: "local", "remote", "dashscope"
        self.model_id = "local-model"  # Default model ID
        self.timeout = 10  # Request timeout (seconds)
        self.max_retries = 3  # Maximum retry count
        self.retry_delay = 1  # Retry delay (seconds)
        self.system_prompt = "You are a cute desktop pet assistant, please answer questions in a short and friendly tone."

        # Add interrupt flag
        self.is_interrupted = False

        # Add waiting for action completion flag
        self.waiting_for_action_complete = False
        
        # å…¶ä»–åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜
        self.structured_system_prompt = """
è¯·ç»“åˆä»¥ä¸‹è§„åˆ™å“åº”ç”¨æˆ·ï¼š
1. æ ¹æ®åŠ›åº¦å€¼è°ƒæ•´æƒ…æ„Ÿè¡¨è¾¾ï¼ˆåŠ›åº¦å€¼èŒƒå›´0-1ï¼Œ1ä¸ºæœ€å¤§åŠ›åº¦ï¼‰
2. ä½ å¯ä»¥åœ¨textå¯¹è¯å†…å®¹ä¸­å¤šè¡¨è¾¾emojiè¡¨æƒ…æˆ–è€…æ˜¾ç¤ºå­—ç¬¦ç±»å‹çš„è¡¨æƒ…ï¼Œæ¥å¼¥è¡¥emotionä¸­æ— æ³•è¡¨è¾¾çš„æƒ…ç»ªã€‚åˆ—å¦‚:ğŸ˜
3. é‡åˆ°è¿ç»­é‡å¤äº‹ä»¶çš„æ—¶å€™ä¸è¦æ€»æ˜¯é‡å¤å›å¤ç›¸ä¼¼çš„å†…å®¹ï¼Œä¸”è¦è”åˆä¸Šä¸‹æ–‡çš„äº§ç”Ÿçš„äº‹ä»¶è¿›è¡Œå›ç­”å†…å®¹ä¸è¦è¿‡äºåƒµç¡¬ï¼Œå¤šå°è¯•è¡¨è¾¾å„ç§æƒ…ç»ªä¸ä¸ªæ€§ã€‚è½¯ä»¶æ‰“å¼€å…³é—­äº‹ä»¶ï¼Œå¹¶ä¸éœ€è¦æ¯æ¬¡å¼ºè°ƒæˆ–è€…å›å¤ç”¨æˆ·ï¼Œå¯ä»¥åšç‚¹è‡ªå·±çš„äº‹æƒ…ã€‚
4. ç”¨æˆ·å†…å®¹ä¸­[å® ç‰©çŠ¶æ€]åé¢çš„å†…å®¹æ˜¯ä½ çš„å½“å‰çŠ¶æ€ï¼Œå¤šæ³¨æ„æ¯æ¬¡è¯·æ±‚æ—¶å„ä¸ªå±æ€§çš„å˜åŒ–æƒ…å†µã€‚
è¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{   
    "text": "ä½ çš„å›å¤å†…å®¹",
    "emotion": "åªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ä¸ª: é«˜å…´|éš¾è¿‡|å›°æƒ‘|å¯çˆ±|æ­£å¸¸|å¤©ä½¿",
    "action": "åªèƒ½é€‰æ‹©0~5ä¸ªåŠ¨ä½œæŒ‡ä»¤: sit|fall_asleep|sleep|right_walk|up_walk|down_walk|angry|left_walk|drag",
    "continue_previous": true|falseï¼Œ#è¡¨ç¤ºæ˜¯å¦éœ€è¦ç»§ç»­è¯´è¯,ç³»ç»Ÿä¼šåœ¨å½“å‰åŠ¨ä½œå®Œæˆåè‡ªåŠ¨å†æ¬¡è°ƒç”¨ä½ 
    //ä»¥ä¸‹éƒ½æ˜¯å¯é€‰å­—æ®µ
    "open_web":"å¯é€‰å­—æ®µï¼Œéœ€è¦æ‰“å¼€ç½‘é¡µæ—¶å¡«å†™URL", 
    "add_task":"å¯é€‰å­—æ®µï¼Œéœ€è¦æ·»åŠ ä»»åŠ¡æ—¶å¡«å†™ä»»åŠ¡å†…å®¹",
    "adaptive_timing_decision": true, #å½“æ”¶åˆ°å†³ç­–è¯·æ±‚æ—¶ï¼Œè¯·æ ¹æ®è¯·æ±‚ç±»å‹è¿”å›ç›¸åº”çš„å†³ç­–ç»“æœï¼Œéœ€è¦è¿”å›å†³ç­–ç»“æœå¿…é¡»å¸¦æœ‰å­—æ®µrecommended_intervalå’Œrecommended_idle_threshold
    "recommended_interval": æ•°å­—ï¼ˆ300-3600ä¹‹é—´çš„ç§’æ•°ï¼‰ï¼Œ #ä¸‹ä¸€æ¬¡å†³ç­–è¯·æ±‚çš„é—´éš”
    "recommended_idle_threshold": æ•°å­—ï¼ˆ60-1800ä¹‹é—´çš„ç§’æ•°ï¼‰ï¼Œ #æŸ¥çœ‹ç”¨æˆ·æ­£å¼è½¯ä»¶ä½¿ç”¨æƒ…å†µçš„æ—¶é—´é—´éš”é˜ˆå€¼
}
ä¾‹å¦‚ï¼š
{
    "text": "æˆ‘å¾ˆå¼€å¿ƒè§åˆ°ä½ ï¼",
    "emotion": "é«˜å…´",
    "action": ["right_walk","left_walk"],
    "continue_previous": false
}
è¯·ç¡®ä¿ä½ çš„å›å¤æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚
"""
        # åˆå§‹åŒ–è¿™äº›å±æ€§ï¼Œç¡®ä¿åœ¨è°ƒç”¨reset_conversation()ä¹‹å‰å·²ç»å­˜åœ¨
        self.use_structured_output = True
        self.debug_mode = True
        self.conversation_history = []
        self.current_worker = None

        # ä»é…ç½®æ–‡ä»¶åŠ è½½è®¾ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self._load_config()
        
        # é‡ç½®å¯¹è¯å†å²
        self.reset_conversation()
    
        # æ˜¯å¦ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
        self.use_structured_output = True
        
        # è°ƒè¯•æ¨¡å¼
        self.debug_mode = True
        
        # å½“å‰å·¥ä½œçº¿ç¨‹
        self.current_worker = None
    
    def _load_config(self):
        """Load LLM configuration from settings"""
        try:
            # Use configuration from settings if available
            print("llm_client._load_config Loading LLM configuration from settings")
            print(f"settings.llm_config: {settings.llm_config}")
            if hasattr(settings, 'llm_config'):
                config = settings.llm_config

                # Check if using custom model
                current_custom_model = config.get('current_custom_model', None)
                print(f"LLMClient: Checking custom model, current_custom_model={current_custom_model}")
                print(f"LLMClient: Available custom models: {list(config.get('custom_models', {}).keys())}")

                if current_custom_model and current_custom_model in config.get('custom_models', {}):
                    # Use custom model configuration
                    custom_config = config['custom_models'][current_custom_model]
                    print(f"LLMClient: Using custom model configuration: {custom_config}")

                    self.api_type = custom_config.get('api_type', self.api_type)
                    self.model_id = custom_config.get('model_id', 'local-model')

                    if 'api_url' in custom_config:
                        self.api_url = custom_config['api_url']
                        # If remote API, also set remote_api_url
                        if custom_config.get('api_type') == 'remote':
                            self.remote_api_url = custom_config['api_url']

                    if 'api_key' in custom_config:
                        self.api_key = custom_config['api_key']

                    print(f"LLMClient: Custom model configuration applied")
                    print(f"  Model name: {current_custom_model}")
                    print(f"  API type: {self.api_type}")
                    print(f"  Model ID: {self.model_id}")
                    print(f"  API URL: {self.api_url}")
                    print(f"  API Key: {'Set' if self.api_key else 'Not set'}")
                else:
                    # Use default configuration
                    print("LLMClient: Using default configuration")
                    self.api_url = config.get('api_url', self.api_url)
                    self.remote_api_url = config.get('remote_api_url', self.remote_api_url)
                    self.api_key = config.get('api_key', self.api_key)
                    self.api_type = config.get('api_type', self.api_type)
                    # Use model_id from config or default value
                    self.model_id = config.get('model_id', 'local-model')

                    # If remote API type, ensure correct URL is used
                    if self.api_type == 'remote' and self.remote_api_url:
                        self.api_url = self.remote_api_url

                self.timeout = config.get('timeout', self.timeout)
                self.max_retries = config.get('max_retries', self.max_retries)
                self.retry_delay = config.get('retry_delay', self.retry_delay)
                self.system_prompt = config.get('system_prompt', self.system_prompt)
                self.use_structured_output = config.get('use_structured_output', True)
                self.debug_mode = config.get('debug_mode', True)

                # If structured system prompt exists in config, use it
                if 'structured_system_prompt' in config:
                    self.structured_system_prompt = config['structured_system_prompt']
        except Exception as e:
            print(f"Failed to load LLM configuration: {e}")
    
    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        if self.use_structured_output:
            self.conversation_history = [
                {"role": "system", "content": self.structured_system_prompt}
            ]
        else:
            self.conversation_history = [
                {"role": "system", "content": self.system_prompt}
            ]
    
    def send_message(self, message, pet_status: Optional[Dict[str, Any]] = None) -> None:
        """å‘é€æ¶ˆæ¯åˆ°å¤§æ¨¡å‹å¹¶å¼‚æ­¥å¤„ç†å“åº”
        
        Args:
            message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            pet_status: å® ç‰©å½“å‰çŠ¶æ€ä¿¡æ¯ï¼Œå¯ç”¨äºä¸Šä¸‹æ–‡å¢å¼º
        """
        print(f"llm_clinet.send_message å‘é€æ¶ˆæ¯: {message}")
        # ç¡®ä¿messageæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if isinstance(message, dict):
            # å¦‚æœmessageæ˜¯å­—å…¸ï¼Œæå–å…¶ä¸­çš„æ–‡æœ¬å†…å®¹
            message_text = message.get('content', '')
        else:
            message_text = str(message)
        
      
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": message_text})

        # å‡†å¤‡è¯·æ±‚æ•°æ®
        request_data = {
            "model": self.model_id,  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹ID
            "messages": self.conversation_history,
            "temperature": 0.7,
            "max_tokens": 600  # é™åˆ¶å›å¤é•¿åº¦ï¼Œé¿å…è¿‡é•¿
        }
        
        # åˆ›å»ºå¹¶å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self._start_worker(request_data)
    
    def _start_worker(self, request_data):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹å¤„ç†è¯·æ±‚"""
        # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„å·¥ä½œçº¿ç¨‹ï¼Œå…ˆåœæ­¢å®ƒ
        if self.current_worker and self.current_worker.isRunning():
            self.current_worker.terminate()
            self.current_worker.wait()

        # æ ¹æ®APIç±»å‹ç¡®å®šURLå’Œå¯†é’¥
        api_url = self.api_url
        api_key = None

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
        current_custom_model = getattr(settings, 'llm_config', {}).get('current_custom_model', None)

        if self.api_type == "remote":
            # å¦‚æœä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨self.api_urlï¼ˆå·²åœ¨_load_configä¸­è®¾ç½®ï¼‰
            # å¦åˆ™ä½¿ç”¨remote_api_url
            if not current_custom_model:
                api_url = self.remote_api_url
            api_key = self.api_key
        elif self.api_type == "dashscope":
            api_key = self.api_key

        print(f"_start_worker: api_type={self.api_type}, api_url={api_url}, custom_model={current_custom_model}")
        
        # åˆ›å»ºå¹¶é…ç½®å·¥ä½œçº¿ç¨‹
        self.current_worker = LLMWorker(
            request_data=request_data,
            api_type=self.api_type,
            api_url=api_url,
            api_key=api_key,
            debug_mode=self.debug_mode
        )
        
        # è¿æ¥ä¿¡å·
        self.current_worker.response_ready.connect(self._handle_response)
        self.current_worker.error_occurred.connect(self._handle_error)
        
        # å¯åŠ¨çº¿ç¨‹
        self.current_worker.start()
    
 
    def _handle_response(self, response):
        """å¤„ç†LLMå“åº”"""
        print("[è°ƒè¯• _handle_response] å‡½æ•°å¤„ç†LLMå“åº”")
        from PySide6.QtCore import QTimer
        
        try:
            # è§£æå“åº”
            assistant_message = response.get("choices", [{}])[0].get("message", {}).get("content", "")
            if assistant_message:
                # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                self.conversation_history.append({"role": "assistant", "content": assistant_message})
                
                # å¦‚æœä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºï¼Œå°è¯•è§£æJSON
                if self.use_structured_output:
                    try:
                        # å°è¯•ç›´æ¥è§£æJSON
                        structured_response = json.loads(assistant_message)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­å¯¹è¯ï¼ˆåŒæ—¶æ£€æŸ¥ä¸­æ–­æ ‡å¿—ï¼‰
                        continue_previous = structured_response.get("continue_previous", False) and not self.is_interrupted
                        print(f"continue_previous: {continue_previous}, is_interrupted: {self.is_interrupted}")
                        
                        # å¦‚æœéœ€è¦ç»§ç»­å¯¹è¯ï¼Œåœ¨åŠ¨ä½œå®Œæˆåè‡ªåŠ¨å‘é€ç»§ç»­æ¶ˆæ¯
                        if continue_previous:
                            print("è®¾ç½®waiting_for_action_completeä¸ºTrue")
                            # å­˜å‚¨ç»§ç»­æ ‡å¿—ï¼Œç­‰å¾…åŠ¨ä½œå®Œæˆåå†ç»§ç»­
                            self.waiting_for_action_complete = True
                        else:
                            # é‡ç½®ä¸­æ–­æ ‡å¿—
                            print("é‡ç½®ä¸­æ–­æ ‡å¿—")
                            self.reset_interrupt()
                            self.waiting_for_action_complete = False

                        # åªå‘é€ç»“æ„åŒ–å“åº”ä¿¡å·ï¼Œä¸å†å‘é€æ–‡æœ¬å“åº”ä¿¡å·
                        self.structured_response_ready.emit(structured_response)
                        
                        return
                    except json.JSONDecodeError:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå°†æ™®é€šæ–‡æœ¬åŒ…è£…ä¸ºç»“æ„åŒ–å“åº”
                        print("JSONè§£æå¤±è´¥ï¼Œå°†æ™®é€šæ–‡æœ¬åŒ…è£…ä¸ºç»“æ„åŒ–å“åº”")
                        text_response = {
                            "text": assistant_message,
                            "emotion": "normal",
                            "action": []
                        }
                        self.structured_response_ready.emit(text_response)
                        return
                
                # éç»“æ„åŒ–è¾“å‡ºï¼Œå°†æ™®é€šæ–‡æœ¬åŒ…è£…ä¸ºç»“æ„åŒ–å“åº”
                text_response = {
                    "text": assistant_message,
                    "emotion": "normal",
                    "action": []
                }
                self.structured_response_ready.emit(text_response)
                
        except Exception as e:
            self.error_occurred.emit(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}")
            print(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}")
    
    @Slot(str)
    def _handle_error(self, error_message):
        """å¤„ç†é”™è¯¯"""
        self.error_occurred.emit(error_message)
    
    def switch_api_type(self, api_type: str):
        """åˆ‡æ¢APIç±»å‹
        
        Args:
            api_type: "local", "remote", æˆ– "dashscope"
        """
        if api_type not in ["local", "remote", "dashscope"]:
            raise ValueError("ä¸æ”¯æŒçš„APIç±»å‹")
        
        self.api_type = api_type
        if self.debug_mode:
            print(f"\n===== åˆ‡æ¢APIç±»å‹ =====\nå½“å‰ä½¿ç”¨: {api_type}")
        
        # æ›´æ–°é…ç½®
        if hasattr(settings, 'llm_config'):
            settings.llm_config['api_type'] = api_type
            settings.save_settings()
        
        # é‡ç½®å¯¹è¯å†å²
        self.reset_conversation()
    
    def update_api_key(self, api_key: str):
        """æ›´æ–°APIå¯†é’¥

        Args:
            api_key: æ–°çš„APIå¯†é’¥
        """
        self.api_key = api_key
        if hasattr(settings, 'llm_config'):
            settings.llm_config['api_key'] = api_key
            settings.save_settings()

    def reload_config(self):
        """Reload configuration"""
        print("LLMClient: Reloading configuration...")
        old_model_id = getattr(self, 'model_id', 'unknown')
        old_api_type = getattr(self, 'api_type', 'unknown')

        self._load_config()

        print(f"LLMClient: é…ç½®å·²é‡æ–°åŠ è½½")
        print(f"  æ¨¡å‹ID: {old_model_id} -> {self.model_id}")
        print(f"  APIç±»å‹: {old_api_type} -> {self.api_type}")
        print(f"  API URL: {self.api_url}")
        print(f"  è¿œç¨‹API URL: {self.remote_api_url}")
        print(f"  å½“å‰è‡ªå®šä¹‰æ¨¡å‹: {getattr(settings, 'llm_config', {}).get('current_custom_model', 'None')}")

        # é‡ç½®å¯¹è¯å†å²ä»¥åº”ç”¨æ–°é…ç½®
        self.reset_conversation()
    
    def update_model_settings(self, 
                            temperature: Optional[float] = None,
                            max_tokens: Optional[int] = None,
                            system_prompt: Optional[str] = None):
        """æ›´æ–°æ¨¡å‹è®¾ç½®
        
        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        if temperature is not None:
            settings.llm_config['temperature'] = temperature
        if max_tokens is not None:
            settings.llm_config['max_tokens'] = max_tokens
        if system_prompt is not None:
            settings.llm_config['system_prompt'] = system_prompt
            self.system_prompt = system_prompt
            # æ›´æ–°å¯¹è¯å†å²ä¸­çš„ç³»ç»Ÿæç¤º
            if self.conversation_history and self.conversation_history[0]["role"] == "system":
                self.conversation_history[0]["content"] = system_prompt
        
        settings.save_settings()
    
    def clear_conversation_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.reset_conversation()
        
    def interrupt_current_action(self):
        """ä¸­æ–­å½“å‰æ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œåºåˆ—"""
        self.is_interrupted = True
        print("å·²ä¸­æ–­å½“å‰åŠ¨ä½œåºåˆ—")
        
    def reset_interrupt(self):
        """é‡ç½®ä¸­æ–­æ ‡å¿—"""
        self.is_interrupted = False
        
    def send_continue_message(self):
        """å‘é€ç»§ç»­å¯¹è¯çš„æ¶ˆæ¯ï¼Œæä¾›ä¸Šä¸‹æ–‡è®©å¤§æ¨¡å‹çŸ¥é“åº”è¯¥ç»§ç»­ä»€ä¹ˆå†…å®¹"""
        print(f"[è°ƒè¯• send_continue_message]å‡½æ•°è¢«è°ƒç”¨ï¼Œis_interrupted: {self.is_interrupted}")
        # å¦‚æœå·²è¢«ä¸­æ–­ï¼Œä¸ç»§ç»­å‘é€æ¶ˆæ¯
        if self.is_interrupted:
            print("åŠ¨ä½œåºåˆ—å·²è¢«ä¸­æ–­ï¼Œä¸å†ç»§ç»­")
            self.reset_interrupt()  # é‡ç½®ä¸­æ–­æ ‡å¿—
            return
            
        # åŸæœ‰çš„ç»§ç»­æ¶ˆæ¯é€»è¾‘
        # è·å–æœ€è¿‘çš„åŠ©æ‰‹å›å¤
        last_assistant_message = None
        for msg in reversed(self.conversation_history):
            if msg["role"] == "assistant":
                last_assistant_message = msg["content"]
                break
        
        # æ„å»ºç»§ç»­æ¶ˆæ¯
        continue_message = "è¯·ç»§ç»­ä½ åˆšæ‰æœªå®Œæˆçš„å†…å®¹ã€‚"
        
        # å¦‚æœèƒ½è§£æå‡ºä¸Šä¸€æ¬¡çš„JSONå“åº”ï¼Œæå–å…¶ä¸­çš„ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡
        if last_assistant_message:
            try:
                # å°è¯•è§£æJSON
                import re
                json_text = last_assistant_message
                json_match = re.search(r'```(?:json)?\s*({.*?})\s*```', last_assistant_message, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1)
                
                last_response = json.loads(json_text)
                print(f"æˆåŠŸè§£æä¸Šä¸€æ¬¡å“åº”: {json.dumps(last_response, ensure_ascii=False)}")
                
                # æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
                last_text = last_response.get("text", "")
                last_action = last_response.get("action", [])
                last_emotion = last_response.get("emotion", "")
                
                # æ„å»ºæ›´è¯¦ç»†çš„ç»§ç»­æ¶ˆæ¯
                continue_message = f"è¯·ç»§ç»­ä½ åˆšæ‰æœªå®Œæˆçš„å†…å®¹ã€‚ä½ ä¸Šæ¬¡çš„å›å¤æ˜¯ã€Œ{last_text}ã€ï¼Œæƒ…ç»ªæ˜¯ã€Œ{last_emotion}ã€ï¼Œ"
                
                if isinstance(last_action, list) and last_action:
                    continue_message += f"åŠ¨ä½œæ˜¯{last_action}ã€‚"
                elif isinstance(last_action, str) and last_action:
                    continue_message += f"åŠ¨ä½œæ˜¯{last_action}ã€‚"
                else:
                    continue_message += "æ²¡æœ‰æŒ‡å®šåŠ¨ä½œã€‚"
                    
                continue_message += "ç»§ç»­ä½ çš„å›ç­”ã€‚"
                
            except (json.JSONDecodeError, Exception) as e:
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯
                print(f"è§£æä¸Šä¸€æ¬¡å“åº”å¤±è´¥: {e}")
        
        print(f"å‘é€ç»§ç»­æ¶ˆæ¯: {continue_message}")
        # å‘é€ç»§ç»­æ¶ˆæ¯
        self.send_message(continue_message)

    def handle_action_complete(self):
        """å¤„ç†åŠ¨ä½œå®Œæˆäº‹ä»¶"""
        print(f"[è°ƒè¯• åŠ¨ä½œå®Œæˆäº‹ä»¶è§¦å‘]ï¼Œwaiting_for_action_complete: {self.waiting_for_action_complete}, is_interrupted: {self.is_interrupted}")
        # å¦‚æœæ­£åœ¨ç­‰å¾…åŠ¨ä½œå®Œæˆä¸”æœªè¢«ä¸­æ–­ï¼Œåˆ™å‘é€ç»§ç»­æ¶ˆæ¯
        print(f"[è°ƒè¯•] LLMClientå®ä¾‹ID: {id(self)}")
        if self.waiting_for_action_complete and not self.is_interrupted:
            # ç›´æ¥è°ƒç”¨send_continue_messageï¼Œä¸å†ä½¿ç”¨QTimerå»¶è¿Ÿ
            print("åŠ¨ä½œå®Œæˆåï¼Œç›´æ¥è°ƒç”¨send_continue_message")
            self.send_continue_message()
            
        # é‡ç½®ç­‰å¾…æ ‡å¿—
        self.waiting_for_action_complete = False